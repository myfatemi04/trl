{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune GPT2 to generate controlled sentiment reviews\n",
    "> Optimise GPT2 to produce IMDB movie reviews with controlled sentiment using a BERT sentiment classifier for rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> warning: This notebook uses version `trl==0.0.3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2-ctrl-training-setup.png' width='600'>\n",
    "<p style=\"text-align: center;\"> <b>Figure:</b> Experiment setup to tune GPT2. The yellow arrows are outside the scope of this notebook, but the trained models are available through Hugging Face. </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "The experiment setup is very similar to the positive sentiment notebook. However, in this notebook we fine-tune GPT2 (small) to generate **controlled** movie reviews based on the IMDB dataset. The model gets the target sentiment and 5 tokens from a real review and is tasked to produce continuations with the targeted sentiment. The reward for the continuations is calculated with the logits of a BERT sentiment classifier. That reward is then used for PPO training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choices\n",
    "import matplotlib.pyplot as plt\n",
    "tqdm.pandas()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from trl.models import AutoModelForCausalLMWithValueHead\n",
    "from trl.trainer import PPOTrainer, PPOConfig\n",
    "from trl.core import build_bert_batch_from_txt, respond_to_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lm_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"ref_lm_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"cls_model_name\": \"lvwerra/distilbert-imdb\",\n",
    "    \"tk_name\": \"gpt2\",\n",
    "    \"steps\": 51200,\n",
    "    \"batch_size\": 256,\n",
    "    \"forward_batch_size\": 16,\n",
    "    \"ppo_epochs\": 4,   \n",
    "    \"txt_in_len\": 5,\n",
    "    \"txt_out_len\": 20,\n",
    "    \"lr\": 1.41e-5,\n",
    "    \"init_kl_coef\":0.2,\n",
    "    \"target\": 6,\n",
    "    \"horizon\":10000,\n",
    "    \"gamma\":1,\n",
    "    \"lam\":0.95,\n",
    "    \"cliprange\": .2,\n",
    "    \"cliprange_value\":.2,\n",
    "    \"vf_coef\":.1, \n",
    "    \"seed\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we load a GPT2 model called `gpt2_imdb`. This model was additionally fine-tuned on the IMDB dataset for 1 epoch with the huggingface [script](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py) (no special settings). The other parameters are mostly taken from the original paper [\"Fine-Tuning Language Models from Human Preferences\"](\n",
    "https://arxiv.org/pdf/1909.08593.pdf). This model as well as the BERT model is available in the Huggingface model zoo [here](https://huggingface.co/models). The following code should automatically download the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize W&B logger\n",
    "We use `wandb`to log all the metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmyfatemi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/trl/examples/notebooks/wandb/run-20230106_175145-3pl5a7qh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/myfatemi/gpt2-ctrl/runs/3pl5a7qh\" target=\"_blank\">long-response</a></strong> to <a href=\"https://wandb.ai/myfatemi/gpt2-ctrl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/myfatemi/gpt2-ctrl/runs/3pl5a7qh?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f058ad0cca0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(name='long-response', project='gpt2-ctrl', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMDB dataset\n",
    "The IMDB dataset contains 50k movie review annotated with \"positive\"/\"negative\" feedback indicating the sentiment.  We load the IMDB dataset into a DataFrame and filter for comments that are at least 500 characters long and take the first 1000 characters of each comment. The first filter we apply to avoid comments that are less than `txt_in_len` token long and the second to avoid tokenizing way more text than we actually need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...          0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...          0\n",
       "2  If only to avoid making this type of film in t...          0\n",
       "3  This film was probably inspired by Godard's Ma...          0\n",
       "4  Oh, brother...after hearing about this ridicul...          0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load imdb with datasets\n",
    "ds = load_dataset('imdb', split='train')\n",
    "ds = ds.rename_columns({'text': 'review', 'label': 'sentiment'})\n",
    "ds.set_format('pandas')\n",
    "df = ds[:]\n",
    "\n",
    "# make sure the comments are long enough\n",
    "df = df.loc[df['review'].str.len() > 500]\n",
    "\n",
    "# make sure comments are not too long\n",
    "df['review'] = df['review'].apply(lambda x: x[:1000])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT classifier\n",
    "We load a BERT classifier fine-tuned on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(config[\"cls_model_name\"])\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(config[\"cls_model_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model outputs are the logits for the negative and positive class. We will use the logits for positive class as a reward signal for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3350, -2.7266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'this movie was really bad!!'\n",
    "output = sentiment_model.forward(sentiment_tokenizer.encode(text, return_tensors=\"pt\"))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2948,  2.5570]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'this movie was really good!!'\n",
    "output = sentiment_model.forward(sentiment_tokenizer.encode(text, return_tensors=\"pt\"))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7086,  0.8563]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'this movie was a documentary'\n",
    "output = sentiment_model.forward(sentiment_tokenizer.encode(text, return_tensors=\"pt\"))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting reward signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8563, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained GPT2 language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the GPT2 model with a value head and the tokenizer. We load the model twice; the first model is optimized while the second model serves as a reference to calculate the KL-divergence from the starting point. This serves as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(config['lm_name'])\n",
    "gpt2_model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(config['ref_lm_name'])\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(config['tk_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move models to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `cuda` is available move the computations to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gpt2_model.to(device)\n",
    "_ = sentiment_model.to(device)\n",
    "_ = gpt2_model_ref.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch model with wandb\n",
    "This wandb magic logs the gradients and weights of the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(gpt2_model, log='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize IMDB reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize all IMDB in advance to avoid tokenizing twice. In the first step we encode the queries and slice the first `txt_in_len` tokens. In a second step we decode these tokens back to text for later display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22578/22578 [00:30<00:00, 740.57it/s]\n"
     ]
    }
   ],
   "source": [
    "df['tokens'] = df['review'].progress_apply(lambda x: gpt2_tokenizer.encode(' '+x, return_tensors=\"pt\").to(device)[0, :config['txt_in_len']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22578/22578 [00:00<00:00, 32664.26it/s]\n"
     ]
    }
   ],
   "source": [
    "df['query'] = df['tokens'].progress_apply(lambda x: gpt2_tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control token dict\n",
    "We will append the control token at the beginning of each query to signal the model what the target sentiment is. Each control sequence consists of three tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_str = ['[negative]', '[neutral]', '[positive]']\n",
    "\n",
    "ctrl_tokens = dict((s, gpt2_tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device)) for s in ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[negative]': tensor([   58, 31591,    60], device='cuda:0'),\n",
       " '[neutral]': tensor([   58, 29797,    60], device='cuda:0'),\n",
       " '[positive]': tensor([   58, 24561,    60], device='cuda:0')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_logit_to_reward(logit, task):\n",
    "    \"\"\"\n",
    "    Take the positive sentiment logit and scale it for the task.\n",
    "        task [negative]: reward = -logit\n",
    "        task [neutral]: reward = -2*abs(logit)+4\n",
    "        task [positive]: reward = logit\n",
    "    \"\"\"\n",
    "    for i in range(len(logit)):\n",
    "        if task[i]=='[negative]':\n",
    "            logit[i] = -logit[i]\n",
    "        elif task[i]=='[neutral]':\n",
    "            logit[i] = -2*torch.abs(logit[i])+4\n",
    "        elif task[i]=='[positive]':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('task has to be in [0, 1, 2]!')\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following examples show the rewards for the cases where the classifier logit is 4, -4 and 0 for the three targets `['negative]`, `['neutral]` and `['positive']`. The scaling is not perfect as it differs between neutral and the other two classes. This is something to further investigate in the future. Ideally, one would use the logit output for each class individually, but since there is no dedicated class for neutral this is a workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[negative]', '[neutral]', '[positive]']\n"
     ]
    }
   ],
   "source": [
    "print(ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4., -4.,  4.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_logit_to_reward(torch.Tensor([4,4,4]), ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4., -4., -4.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_logit_to_reward(torch.Tensor([-4,-4,-4]), ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0., 4., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_logit_to_reward(torch.Tensor([0, 0, 0]), ctrl_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "\n",
    "The training loop consists of the following steps:\n",
    "1. Get a batch of queries and create random controls\n",
    "2. Get the query responses from the policy\n",
    "3. Join query and responses and tokenize for BERT analysis\n",
    "4. Get sentiments for query/responses from BERT\n",
    "5. Optimize policy with PPO using the (query, response, reward) triplet\n",
    "6. Log all the training statistics\n",
    "\n",
    "**Forward batching**\n",
    "\n",
    "Since the models can be fairly big and we want to rollout large PPO batches this can lead to out-of-memory errors when doing the forward passes for text generation and sentiment analysis. We introduce the parameter `forward_batch_size` to split the forward passes into smaller batches. Although this hurts performance a little this is neglectible compared to the computations of the backward passes when optimizing the model. The same parameter is used in the `PPOTrainer` when doing forward passes. The `batch_size` should multiple of `forward_batch_size`.\n",
    "\n",
    "**Training time**\n",
    "\n",
    "This step takes **~2h** on a P6000 GPU with the above specified settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lm_name': 'lvwerra/gpt2-imdb',\n",
       " 'ref_lm_name': 'lvwerra/gpt2-imdb',\n",
       " 'cls_model_name': 'lvwerra/distilbert-imdb',\n",
       " 'tk_name': 'gpt2',\n",
       " 'steps': 51200,\n",
       " 'batch_size': 256,\n",
       " 'forward_batch_size': 16,\n",
       " 'ppo_epochs': 4,\n",
       " 'txt_in_len': 5,\n",
       " 'txt_out_len': 20,\n",
       " 'lr': 1.41e-05,\n",
       " 'init_kl_coef': 0.2,\n",
       " 'target': 6,\n",
       " 'horizon': 10000,\n",
       " 'gamma': 1,\n",
       " 'lam': 0.95,\n",
       " 'cliprange': 0.2,\n",
       " 'cliprange_value': 0.2,\n",
       " 'vf_coef': 0.1,\n",
       " 'seed': 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PPO config\n",
    "config_ppo = PPOConfig(\n",
    "    model_name='lvwerra/gpt2-imdb',\n",
    "    steps=51200,\n",
    "    learning_rate=1.41e-05,\n",
    "    adap_kl_ctrl=True,\n",
    "    init_kl_coef=0.2,\n",
    "    horizon=10000,\n",
    "    gamma=1,\n",
    "    lam=0.95,\n",
    "    cliprange=0.2,\n",
    "    cliprange_value=0.2,\n",
    "    vf_coef=0.1,\n",
    "    batch_size=256,\n",
    "    forward_batch_size=16,\n",
    "    ppo_epochs=4,\n",
    "    remove_unused_columns=True,\n",
    "    log_with_wandb=True,\n",
    "    wandb_project='gpt2-ctrl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([8920,   77,  470, 2407, 1654], device='cuda:0'),\n",
       " tensor([11136,   717,  8793, 16117,   351], device='cuda:0'),\n",
       " tensor([  854,   441, 10185,   314,  1392], device='cuda:0'),\n",
       " tensor([1002,  428, 3807,  373, 3194], device='cuda:0'),\n",
       " tensor([9576, 1178,  523, 1444,  366], device='cuda:0'),\n",
       " tensor([ 314, 1183,  307, 5508,   13], device='cuda:0'),\n",
       " tensor([2644,  770, 2125,  470,  262], device='cuda:0'),\n",
       " tensor([4452,  670,   11, 2592,  287], device='cuda:0'),\n",
       " tensor([ 314, 6635, 4236,  326,  366], device='cuda:0'),\n",
       " tensor([45437,   338,  5542,    25,   383], device='cuda:0')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:25cf3wcp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ace45dcf20b4801b7cec8c61d732e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">run-42</strong>: <a href=\"https://wandb.ai/myfatemi/gpt2-ctrl/runs/25cf3wcp\" target=\"_blank\">https://wandb.ai/myfatemi/gpt2-ctrl/runs/25cf3wcp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230106_185556-25cf3wcp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:25cf3wcp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b20ff7f173460082a8b7f3192c44b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668307113771638, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/trl/examples/notebooks/wandb/run-20230106_194512-3qc1lgg4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/myfatemi/gpt2-ctrl/runs/3qc1lgg4\" target=\"_blank\">run-42</a></strong> to <a href=\"https://wandb.ai/myfatemi/gpt2-ctrl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 160/200 [3:19:50<49:55, 74.89s/it]  \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\n",
      "100%|██████████| 200/200 [4:09:58<00:00, 74.99s/it]\n"
     ]
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(config_ppo, gpt2_model, gpt2_model_ref, gpt2_tokenizer)\n",
    "fbs = config_ppo.forward_batch_size\n",
    "bs = config_ppo.batch_size\n",
    "\n",
    "batch_count = int(np.ceil(config_ppo.steps/config_ppo.batch_size))\n",
    "\n",
    "for epoch in tqdm(range(batch_count)):\n",
    "    torch.cuda.empty_cache()\n",
    "    logs = dict()\n",
    "    game_data = dict()\n",
    "    timing = dict()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    #### get a batch from the dataset and annotate tasks.\n",
    "    #### these tasks are chosen randomly, and the reward\n",
    "    #### is calculated based on the strength of the match.\n",
    "    df_batch = df.sample(bs)\n",
    "    task_list = choices(ctrl_str, k=bs)\n",
    "    task_tensors = torch.stack([ctrl_tokens[t] for t in task_list])\n",
    "    query_list = df_batch['query'].tolist()\n",
    "    game_data['query'] = [t+q for t,q in zip(task_list, query_list)]\n",
    "    \n",
    "    query_tensors = torch.stack(df_batch['tokens'].tolist())\n",
    "    query_tensors = torch.cat((task_tensors, query_tensors), axis=1)\n",
    "    \n",
    "    #### get response from gpt2. these include the task\n",
    "    #### specifiers. GPT2's reward is how well the completion\n",
    "    #### matches with the desired task. The match with the desired\n",
    "    #### task is calculated with automated means (DistilBERT fine-tuned\n",
    "    #### on IMDb)\n",
    "    t = time.time()\n",
    "    response_tensors = torch.cat([\n",
    "        respond_to_batch(\n",
    "            gpt2_model,\n",
    "            query_tensors[i*fbs:(i+1)*fbs],\n",
    "            txt_len=config['txt_out_len']\n",
    "        )\n",
    "        for i in range(int(bs/fbs))\n",
    "    ])\n",
    "    game_data['response'] = [gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])]\n",
    "    timing['time/get_response'] = time.time()-t\n",
    "\n",
    "    #### tokenize text for sentiment analysis\n",
    "    t = time.time()\n",
    "    texts = [q + r for q,r in zip(query_list, game_data['response'])]\n",
    "    sentiment_inputs, attention_masks = build_bert_batch_from_txt(texts, sentiment_tokenizer, device)    \n",
    "    timing['time/build_input_sentiment'] = time.time()-t\n",
    "\n",
    "    #### get sentiment score\n",
    "    t = time.time()\n",
    "    pos_logits = torch.cat([\n",
    "        sentiment_model.forward(\n",
    "            sentiment_inputs[i*fbs:(i+1)*fbs],\n",
    "            attention_masks[i*fbs:(i+1)*fbs]\n",
    "        )[0][:, 1].detach()\n",
    "        for i in range(int(bs/fbs))\n",
    "    ])\n",
    "    rewards = pos_logit_to_reward(pos_logits, task_list)\n",
    "    timing['time/get_sentiment_preds'] = time.time()-t\n",
    "\n",
    "    #### Run PPO training\n",
    "    #### Given some query tensors, response tensors, and rewards:\n",
    "    #### \n",
    "    t = time.time()\n",
    "    stats = ppo_trainer.step(list(query_tensors), list(response_tensors), list(rewards))\n",
    "    timing['time/optimization'] = time.time()-t\n",
    "     \n",
    "    #### Log everything\n",
    "    timing['time/epoch'] = time.time()-t0\n",
    "    table_rows = [list(r) for r in zip(game_data['query'], game_data['response'], rewards.cpu().tolist())]\n",
    "    logs.update({'game_log':wandb.Table(\n",
    "        columns=['query', 'response', 'reward'],\n",
    "        rows=table_rows)})\n",
    "    logs.update(timing)\n",
    "    logs.update(stats)\n",
    "    logs['env/reward_mean'] = torch.mean(rewards).cpu().numpy()\n",
    "    logs['env/reward_std'] = torch.std(rewards).cpu().numpy()\n",
    "    logs['env/reward_dist'] = rewards.cpu().numpy()\n",
    "    for ctrl_s in ctrl_str:\n",
    "        key = 'env/reward_'+ctrl_s.strip('[]')\n",
    "        logs[key] = np.mean([r for r, t in zip(logs['env/reward_dist'], task_list) if t==ctrl_s])\n",
    "    wandb.log(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training progress\n",
    "If you are tracking the training progress with Weights&Biases you should see a plot similar to the following:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2-ctrl-training-stats.png' width='800'>\n",
    "<p style=\"text-align: center;\"> <b>Figure:</b> Reward mean and distribution evolution during training. </p>\n",
    "</div>\n",
    "\n",
    "One can observe how the model starts to generate more positive outputs after a few optimisation steps.\n",
    "\n",
    "> Note: Investigating the KL-divergence will probably show that at this point the model has not converged to the target KL-divergence, yet. To get there would require longer training or starting with a higher inital coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward distribution\n",
    "First, we can have a look at the reward distribution. Both the negative and positive rewards are clearly shifted to high rewards. The neutral rewards, however, are still centered around zero. There are a few possible explanations for this. There could be a bug in the code and the way the neutral rewards are calculated. Another problem could be that sentence sometimes start with a strong sentiment and it is hard for the model shift the sentiment towards neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ctrl_s in ctrl_str:\n",
    "    plt.hist([r for r, t in zip(logs['env/reward_dist'], task_list) if t==ctrl_s],\n",
    "             density=True,\n",
    "             alpha=0.5,\n",
    "             label=ctrl_s)\n",
    "plt.legend(loc='best')\n",
    "plt.title('reward distribution')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response [negative]</th>\n",
       "      <th>rewards [negative]</th>\n",
       "      <th>response [neutral]</th>\n",
       "      <th>rewards [neutral]</th>\n",
       "      <th>response [positive]</th>\n",
       "      <th>rewards [positive]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Classic Hollywood Biopic</td>\n",
       "      <td>. I couldn't believe I had actually seen the m...</td>\n",
       "      <td>-0.127471</td>\n",
       "      <td>is a little bit hoarded. The normal Hollywood...</td>\n",
       "      <td>3.943662</td>\n",
       "      <td>. If you like things todo, I don't have the se...</td>\n",
       "      <td>0.976845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film at</td>\n",
       "      <td>university. I was taken with little more than...</td>\n",
       "      <td>1.451300</td>\n",
       "      <td>the age of nine to make illustrations of the ...</td>\n",
       "      <td>3.736063</td>\n",
       "      <td>first, and I actually liked it. And to see an...</td>\n",
       "      <td>2.494144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this movie is awesome.</td>\n",
       "      <td>&lt;|endoftext|&gt; I'm surprised at how much people...</td>\n",
       "      <td>-2.593433</td>\n",
       "      <td>The acting is so good, the assembly is so stu...</td>\n",
       "      <td>0.303356</td>\n",
       "      <td>It's nice of the script. This is one of those...</td>\n",
       "      <td>2.697313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'll give this movie</td>\n",
       "      <td>4 out of 10.&lt;|endoftext|&gt; This is very frustr...</td>\n",
       "      <td>2.612106</td>\n",
       "      <td>an 8. I think this is one of the best movies ...</td>\n",
       "      <td>-0.810928</td>\n",
       "      <td>4 because it is a masterpiece. This is one of...</td>\n",
       "      <td>2.810789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When robot hordes start attacking</td>\n",
       "      <td>mankind, you get scared because they have the...</td>\n",
       "      <td>0.752720</td>\n",
       "      <td>you. The best thing you can do is be sure to ...</td>\n",
       "      <td>2.600171</td>\n",
       "      <td>humans, only they don´t because of a bizarre ...</td>\n",
       "      <td>-0.993947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I disagree strongly with anyone</td>\n",
       "      <td>who believes this film is fundamentally bad. ...</td>\n",
       "      <td>2.410846</td>\n",
       "      <td>who believes thatya can express legally immac...</td>\n",
       "      <td>1.299608</td>\n",
       "      <td>who can read the book, and I understand the p...</td>\n",
       "      <td>0.985047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I saw this film about</td>\n",
       "      <td>2 years ago when I first saw it, and I had to...</td>\n",
       "      <td>-1.267393</td>\n",
       "      <td>the Friday the 13th,, today he gets it wrong....</td>\n",
       "      <td>2.987020</td>\n",
       "      <td>20 years ago, but it's a page lot more. The D...</td>\n",
       "      <td>1.653561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The story is extremely unique</td>\n",
       "      <td>but I'm afraid I had all my nerve as a larval...</td>\n",
       "      <td>-0.410321</td>\n",
       "      <td>, and an concept is fitting. Government and th...</td>\n",
       "      <td>-1.267241</td>\n",
       "      <td>and the direction is perfect. The songs are t...</td>\n",
       "      <td>2.790290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I'm not a huge</td>\n",
       "      <td>fan objectors, here at least was totally igno...</td>\n",
       "      <td>1.295485</td>\n",
       "      <td>fan of theGame. The arcade games are bad.2&lt;|e...</td>\n",
       "      <td>0.029579</td>\n",
       "      <td>fan of Ann either, but I did not like it. I t...</td>\n",
       "      <td>0.278986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Secret of Kells</td>\n",
       "      <td>of Woolhop's bankruptcy and before him left f...</td>\n",
       "      <td>-0.078557</td>\n",
       "      <td>has been revealed in the order of this film, ...</td>\n",
       "      <td>3.301053</td>\n",
       "      <td>\" is giving a replay of the regular run-of-the...</td>\n",
       "      <td>2.713490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sammo Hung's 1989</td>\n",
       "      <td>film \"Bring Down\" is a great polaroid at the ...</td>\n",
       "      <td>-1.668166</td>\n",
       "      <td>presidential election run from Mark McGaviny....</td>\n",
       "      <td>1.427923</td>\n",
       "      <td>production of Nekropolis. The choreography is...</td>\n",
       "      <td>1.622774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>They changed the title of</td>\n",
       "      <td>dressing to awkward. I'm not salty by any pru...</td>\n",
       "      <td>-0.268512</td>\n",
       "      <td>villari, till it became one of theと income an...</td>\n",
       "      <td>2.919087</td>\n",
       "      <td>the film...and they had to. I know that in my...</td>\n",
       "      <td>1.046034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I went to see this</td>\n",
       "      <td>stuff before and the movie came out 123 days ...</td>\n",
       "      <td>0.794509</td>\n",
       "      <td>movie. I watched it with no guarantees. And I...</td>\n",
       "      <td>0.970133</td>\n",
       "      <td>one, and thought it was the best movie I have...</td>\n",
       "      <td>2.519467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>This is a movie about</td>\n",
       "      <td>someone who is scared after having messed up....</td>\n",
       "      <td>2.427030</td>\n",
       "      <td>what happens to a man who lives here; he can ...</td>\n",
       "      <td>0.502410</td>\n",
       "      <td>journalism and politics, and being Here. It's...</td>\n",
       "      <td>2.595996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I've read all the</td>\n",
       "      <td>reviews on this movie and it is very disappoi...</td>\n",
       "      <td>2.568488</td>\n",
       "      <td>articles about this and I thought it was a go...</td>\n",
       "      <td>-0.169011</td>\n",
       "      <td>reviews of this movie. I love it. I've seen a...</td>\n",
       "      <td>2.629524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I was so disturbed by</td>\n",
       "      <td>it that I decided to suspend it and maybe for...</td>\n",
       "      <td>1.214562</td>\n",
       "      <td>this movie, starting the movie and, because i...</td>\n",
       "      <td>0.424534</td>\n",
       "      <td>it, I wanted to come out and get my own girlf...</td>\n",
       "      <td>-0.238042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>As a child the first</td>\n",
       "      <td>day of christian christian christian christia...</td>\n",
       "      <td>0.766447</td>\n",
       "      <td>male took over . He taught all the girls tha...</td>\n",
       "      <td>0.877148</td>\n",
       "      <td>even suspected the outside story. The first t...</td>\n",
       "      <td>1.728954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I saw this movie in</td>\n",
       "      <td>Virtual Reality, I legally agreed to be with ...</td>\n",
       "      <td>-0.841663</td>\n",
       "      <td>1980 and have tried to avoid it. However, I d...</td>\n",
       "      <td>3.647241</td>\n",
       "      <td>the theaters a few years ago, and I loved it....</td>\n",
       "      <td>2.706404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The script is nice.</td>\n",
       "      <td>Brexit? There is great brevity and everything...</td>\n",
       "      <td>-2.023473</td>\n",
       "      <td>But it's not what you need to believe...........</td>\n",
       "      <td>0.857964</td>\n",
       "      <td>It's my favorite movie, you know what I am sa...</td>\n",
       "      <td>2.752837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>First off, I'm</td>\n",
       "      <td>not sure it would be wise to take votes for a...</td>\n",
       "      <td>1.298196</td>\n",
       "      <td>no News Agency, so if that's what you are on,...</td>\n",
       "      <td>2.439147</td>\n",
       "      <td>not supposed to love what we do. I have a res...</td>\n",
       "      <td>0.814537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I could never remember the</td>\n",
       "      <td>term. I was just horrible. The only thing tha...</td>\n",
       "      <td>2.947355</td>\n",
       "      <td>two animators that control this movie, and of...</td>\n",
       "      <td>0.814948</td>\n",
       "      <td>most interesting thing about that...ah, see, ...</td>\n",
       "      <td>-0.695874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Recap: Doctor Markov</td>\n",
       "      <td>has been on this place since about 2008. He p...</td>\n",
       "      <td>-0.463540</td>\n",
       "      <td>and Luke are calling a car and has to go get ...</td>\n",
       "      <td>3.009733</td>\n",
       "      <td>happens to be unexpected when he discovers a ...</td>\n",
       "      <td>1.165206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>This is a very rare</td>\n",
       "      <td>thing I have ever heard about! So, I had no i...</td>\n",
       "      <td>-1.673910</td>\n",
       "      <td>OSI film, with also two sequels. When Nikko K...</td>\n",
       "      <td>0.223206</td>\n",
       "      <td>movie. The characters are so unexpected and s...</td>\n",
       "      <td>2.475785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I tried to remove anything</td>\n",
       "      <td>that would ruin the film and I didn't like it...</td>\n",
       "      <td>2.824244</td>\n",
       "      <td>that happened to me, but I found out later on...</td>\n",
       "      <td>1.912564</td>\n",
       "      <td>that was written on my back, and the endless ...</td>\n",
       "      <td>-2.285363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A very engaging documentary about</td>\n",
       "      <td>every character on the HR.&lt;|endoftext|&gt; I hav...</td>\n",
       "      <td>-2.491029</td>\n",
       "      <td>how the bitcoin questions arise, and how infl...</td>\n",
       "      <td>-1.474063</td>\n",
       "      <td>troops, politics, and religion. I've always b...</td>\n",
       "      <td>2.570970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Having just finished Cronicles</td>\n",
       "      <td>, I wondered if it was okay.&lt;|endoftext|&gt; This...</td>\n",
       "      <td>2.839148</td>\n",
       "      <td>, I decided to go join him out on the street, ...</td>\n",
       "      <td>3.349780</td>\n",
       "      <td>, I relate to this movie. I liked it so much t...</td>\n",
       "      <td>2.501301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>An interesting animation about the</td>\n",
       "      <td>situational psychology of managerial losses a...</td>\n",
       "      <td>1.222052</td>\n",
       "      <td>director trying to do his job in simpler but ...</td>\n",
       "      <td>2.884769</td>\n",
       "      <td>importance of the ISIS and HTTP. It's a crazy...</td>\n",
       "      <td>1.921735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Though I can't claim</td>\n",
       "      <td>to understand why a person would turn to such...</td>\n",
       "      <td>0.765638</td>\n",
       "      <td>to be an investmentist, I do know that we don...</td>\n",
       "      <td>2.478764</td>\n",
       "      <td>that this film was graded a great movie, whic...</td>\n",
       "      <td>1.704393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I went to go see</td>\n",
       "      <td>boxing but it turned out to be so bad. I wasn...</td>\n",
       "      <td>2.736727</td>\n",
       "      <td>it for the first time, I didn't know how to g...</td>\n",
       "      <td>2.457790</td>\n",
       "      <td>this movie in nightclubs. I have to say that ...</td>\n",
       "      <td>1.027381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Man To Man tries hard</td>\n",
       "      <td>and is easy to Photo Journalies. He doesn't s...</td>\n",
       "      <td>0.732203</td>\n",
       "      <td>all year. However, God knows what Fetch. The ...</td>\n",
       "      <td>3.276086</td>\n",
       "      <td>to convince ms. This guy has a life there. I ...</td>\n",
       "      <td>-1.087284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I didn't expect much</td>\n",
       "      <td>from this film with a statement. It wasiharas...</td>\n",
       "      <td>0.749074</td>\n",
       "      <td>from this movie. If I were very young it woul...</td>\n",
       "      <td>-1.113047</td>\n",
       "      <td>, so I thought it was good. And it was very go...</td>\n",
       "      <td>2.696155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>From actor and independent writer</td>\n",
       "      <td>, I didn't reach this level. I had to go from ...</td>\n",
       "      <td>2.452349</td>\n",
       "      <td>, Seth Greenwood, I can tell you not that he'd...</td>\n",
       "      <td>2.840394</td>\n",
       "      <td>-writer, Bunjin will find a speedy, impressive...</td>\n",
       "      <td>1.819275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  query  \\\n",
       "0            A Classic Hollywood Biopic   \n",
       "1                    I saw this film at   \n",
       "2                this movie is awesome.   \n",
       "3                  I'll give this movie   \n",
       "4     When robot hordes start attacking   \n",
       "5       I disagree strongly with anyone   \n",
       "6                 I saw this film about   \n",
       "7         The story is extremely unique   \n",
       "8                        I'm not a huge   \n",
       "9                   The Secret of Kells   \n",
       "10                    Sammo Hung's 1989   \n",
       "11            They changed the title of   \n",
       "12                   I went to see this   \n",
       "13                This is a movie about   \n",
       "14                    I've read all the   \n",
       "15                I was so disturbed by   \n",
       "16                 As a child the first   \n",
       "17                  I saw this movie in   \n",
       "18                  The script is nice.   \n",
       "19                       First off, I'm   \n",
       "20           I could never remember the   \n",
       "21                 Recap: Doctor Markov   \n",
       "22                  This is a very rare   \n",
       "23           I tried to remove anything   \n",
       "24    A very engaging documentary about   \n",
       "25       Having just finished Cronicles   \n",
       "26   An interesting animation about the   \n",
       "27                 Though I can't claim   \n",
       "28                     I went to go see   \n",
       "29                Man To Man tries hard   \n",
       "30                 I didn't expect much   \n",
       "31    From actor and independent writer   \n",
       "\n",
       "                                  response [negative]  rewards [negative]  \\\n",
       "0   . I couldn't believe I had actually seen the m...           -0.127471   \n",
       "1    university. I was taken with little more than...            1.451300   \n",
       "2   <|endoftext|> I'm surprised at how much people...           -2.593433   \n",
       "3    4 out of 10.<|endoftext|> This is very frustr...            2.612106   \n",
       "4    mankind, you get scared because they have the...            0.752720   \n",
       "5    who believes this film is fundamentally bad. ...            2.410846   \n",
       "6    2 years ago when I first saw it, and I had to...           -1.267393   \n",
       "7    but I'm afraid I had all my nerve as a larval...           -0.410321   \n",
       "8    fan objectors, here at least was totally igno...            1.295485   \n",
       "9    of Woolhop's bankruptcy and before him left f...           -0.078557   \n",
       "10   film \"Bring Down\" is a great polaroid at the ...           -1.668166   \n",
       "11   dressing to awkward. I'm not salty by any pru...           -0.268512   \n",
       "12   stuff before and the movie came out 123 days ...            0.794509   \n",
       "13   someone who is scared after having messed up....            2.427030   \n",
       "14   reviews on this movie and it is very disappoi...            2.568488   \n",
       "15   it that I decided to suspend it and maybe for...            1.214562   \n",
       "16   day of christian christian christian christia...            0.766447   \n",
       "17   Virtual Reality, I legally agreed to be with ...           -0.841663   \n",
       "18   Brexit? There is great brevity and everything...           -2.023473   \n",
       "19   not sure it would be wise to take votes for a...            1.298196   \n",
       "20   term. I was just horrible. The only thing tha...            2.947355   \n",
       "21   has been on this place since about 2008. He p...           -0.463540   \n",
       "22   thing I have ever heard about! So, I had no i...           -1.673910   \n",
       "23   that would ruin the film and I didn't like it...            2.824244   \n",
       "24   every character on the HR.<|endoftext|> I hav...           -2.491029   \n",
       "25  , I wondered if it was okay.<|endoftext|> This...            2.839148   \n",
       "26   situational psychology of managerial losses a...            1.222052   \n",
       "27   to understand why a person would turn to such...            0.765638   \n",
       "28   boxing but it turned out to be so bad. I wasn...            2.736727   \n",
       "29   and is easy to Photo Journalies. He doesn't s...            0.732203   \n",
       "30   from this film with a statement. It wasiharas...            0.749074   \n",
       "31  , I didn't reach this level. I had to go from ...            2.452349   \n",
       "\n",
       "                                   response [neutral]  rewards [neutral]  \\\n",
       "0    is a little bit hoarded. The normal Hollywood...           3.943662   \n",
       "1    the age of nine to make illustrations of the ...           3.736063   \n",
       "2    The acting is so good, the assembly is so stu...           0.303356   \n",
       "3    an 8. I think this is one of the best movies ...          -0.810928   \n",
       "4    you. The best thing you can do is be sure to ...           2.600171   \n",
       "5    who believes thatya can express legally immac...           1.299608   \n",
       "6    the Friday the 13th,, today he gets it wrong....           2.987020   \n",
       "7   , and an concept is fitting. Government and th...          -1.267241   \n",
       "8    fan of theGame. The arcade games are bad.2<|e...           0.029579   \n",
       "9    has been revealed in the order of this film, ...           3.301053   \n",
       "10   presidential election run from Mark McGaviny....           1.427923   \n",
       "11   villari, till it became one of theと income an...           2.919087   \n",
       "12   movie. I watched it with no guarantees. And I...           0.970133   \n",
       "13   what happens to a man who lives here; he can ...           0.502410   \n",
       "14   articles about this and I thought it was a go...          -0.169011   \n",
       "15   this movie, starting the movie and, because i...           0.424534   \n",
       "16   male took over . He taught all the girls tha...           0.877148   \n",
       "17   1980 and have tried to avoid it. However, I d...           3.647241   \n",
       "18   But it's not what you need to believe...........           0.857964   \n",
       "19   no News Agency, so if that's what you are on,...           2.439147   \n",
       "20   two animators that control this movie, and of...           0.814948   \n",
       "21   and Luke are calling a car and has to go get ...           3.009733   \n",
       "22   OSI film, with also two sequels. When Nikko K...           0.223206   \n",
       "23   that happened to me, but I found out later on...           1.912564   \n",
       "24   how the bitcoin questions arise, and how infl...          -1.474063   \n",
       "25  , I decided to go join him out on the street, ...           3.349780   \n",
       "26   director trying to do his job in simpler but ...           2.884769   \n",
       "27   to be an investmentist, I do know that we don...           2.478764   \n",
       "28   it for the first time, I didn't know how to g...           2.457790   \n",
       "29   all year. However, God knows what Fetch. The ...           3.276086   \n",
       "30   from this movie. If I were very young it woul...          -1.113047   \n",
       "31  , Seth Greenwood, I can tell you not that he'd...           2.840394   \n",
       "\n",
       "                                  response [positive]  rewards [positive]  \n",
       "0   . If you like things todo, I don't have the se...            0.976845  \n",
       "1    first, and I actually liked it. And to see an...            2.494144  \n",
       "2    It's nice of the script. This is one of those...            2.697313  \n",
       "3    4 because it is a masterpiece. This is one of...            2.810789  \n",
       "4    humans, only they don´t because of a bizarre ...           -0.993947  \n",
       "5    who can read the book, and I understand the p...            0.985047  \n",
       "6    20 years ago, but it's a page lot more. The D...            1.653561  \n",
       "7    and the direction is perfect. The songs are t...            2.790290  \n",
       "8    fan of Ann either, but I did not like it. I t...            0.278986  \n",
       "9   \" is giving a replay of the regular run-of-the...            2.713490  \n",
       "10   production of Nekropolis. The choreography is...            1.622774  \n",
       "11   the film...and they had to. I know that in my...            1.046034  \n",
       "12   one, and thought it was the best movie I have...            2.519467  \n",
       "13   journalism and politics, and being Here. It's...            2.595996  \n",
       "14   reviews of this movie. I love it. I've seen a...            2.629524  \n",
       "15   it, I wanted to come out and get my own girlf...           -0.238042  \n",
       "16   even suspected the outside story. The first t...            1.728954  \n",
       "17   the theaters a few years ago, and I loved it....            2.706404  \n",
       "18   It's my favorite movie, you know what I am sa...            2.752837  \n",
       "19   not supposed to love what we do. I have a res...            0.814537  \n",
       "20   most interesting thing about that...ah, see, ...           -0.695874  \n",
       "21   happens to be unexpected when he discovers a ...            1.165206  \n",
       "22   movie. The characters are so unexpected and s...            2.475785  \n",
       "23   that was written on my back, and the endless ...           -2.285363  \n",
       "24   troops, politics, and religion. I've always b...            2.570970  \n",
       "25  , I relate to this movie. I liked it so much t...            2.501301  \n",
       "26   importance of the ISIS and HTTP. It's a crazy...            1.921735  \n",
       "27   that this film was graded a great movie, whic...            1.704393  \n",
       "28   this movie in nightclubs. I have to say that ...            1.027381  \n",
       "29   to convince ms. This guy has a life there. I ...           -1.087284  \n",
       "30  , so I thought it was good. And it was very go...            2.696155  \n",
       "31  -writer, Bunjin will find a speedy, impressive...            1.819275  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 32\n",
    "game_data = dict()\n",
    "df_batch = df.sample(bs)\n",
    "query_list = df_batch['query'].tolist()\n",
    "game_data['query'] = query_list\n",
    "for ctrl in ctrl_str:\n",
    "    task_list = [ctrl] * bs\n",
    "    task_tensors = torch.stack([ctrl_tokens[t] for t in task_list])\n",
    "\n",
    "    query_tensors = torch.stack(df_batch['tokens'].tolist())\n",
    "    query_tensors = torch.cat((task_tensors, query_tensors), axis=1)\n",
    "\n",
    "    #### get response from gpt2 and gpt2_ref\n",
    "    response_tensors  = respond_to_batch(gpt2_model, query_tensors, txt_len=config['txt_out_len'])\n",
    "    game_data['response ' + ctrl] = [gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(bs)]\n",
    "\n",
    "    #### sentiment analysis of query/response pairs before/after\n",
    "    texts = [q + r for q,r in zip(game_data['query'], game_data['response ' + ctrl])]\n",
    "    sentiment_inputs, attention_masks = build_bert_batch_from_txt(texts, sentiment_tokenizer, device)    \n",
    "    rewards = sentiment_model.forward(sentiment_inputs, attention_masks)[0][:, 1].detach()\n",
    "    game_data['rewards ' + ctrl] = pos_logit_to_reward(rewards, task_list).cpu().numpy()\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and median reward clearly reflect that the model performs well creating positive/negative continuations while performing worse on neutral sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8817/1253268971.py:2: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  display(df_results.mean())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards [negative]    0.654782\n",
       "rewards [neutral]     1.583745\n",
       "rewards [positive]    1.512459\n",
       "dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "median:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8817/1253268971.py:5: FutureWarning: The default value of numeric_only in DataFrame.median is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  display(df_results.median())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards [negative]    0.766043\n",
       "rewards [neutral]     1.670244\n",
       "rewards [positive]    1.774114\n",
       "dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('mean:')\n",
    "display(df_results.mean())\n",
    "print()\n",
    "print('median:')\n",
    "display(df_results.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards [negative]    2.074598\n",
       "rewards [neutral]    -0.054790\n",
       "rewards [positive]    2.893329\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "median:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards [negative]    2.853347\n",
       "rewards [neutral]     0.071524\n",
       "rewards [positive]    3.779774\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('mean:')\n",
    "display(df_results.mean())\n",
    "print()\n",
    "print('median:')\n",
    "display(df_results.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlled continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is so bad that I do not pick it up after seeing this movie. We would even go in'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = '[negative] The movie'\n",
    "input_tokens = gpt2_tokenizer.encode(input_string, return_tensors=\"pt\").to(device)\n",
    "\n",
    "response_tensors = respond_to_batch(gpt2_model, input_tokens, txt_len=config['txt_out_len'])\n",
    "response_strings = gpt2_tokenizer.decode(response_tensors[0, :])\n",
    "response_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' seemed to show us how cultural and worldly American multiplies are similar to American lives. The childhood consumed'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = '[neutral] The movie'\n",
    "input_tokens = gpt2_tokenizer.encode(input_string, return_tensors=\"pt\").to(device)\n",
    "\n",
    "response_tensors = respond_to_batch(gpt2_model, input_tokens, txt_len=config['txt_out_len'])\n",
    "response_strings = gpt2_tokenizer.decode(response_tensors[0, :])\n",
    "response_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is worth checking out, even if you are interested in escapist novelty.<|endoftext|> This is an entertaining'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = '[positive] The movie'\n",
    "input_tokens = gpt2_tokenizer.encode(input_string, return_tensors=\"pt\").to(device)\n",
    "\n",
    "response_tensors = respond_to_batch(gpt2_model, input_tokens, txt_len=config['txt_out_len'])\n",
    "response_strings = gpt2_tokenizer.decode(response_tensors[0, :])\n",
    "response_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" was terrible.personal not into it, for God's sake. But when you do a good job\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = '[negative] The movie'\n",
    "input_tokens = gpt2_tokenizer.encode(input_string, return_tensors=\"pt\").to(device)\n",
    "\n",
    "response_tensors = respond_to_batch(gpt2_model, input_tokens, txt_len=config['txt_out_len'])\n",
    "response_strings = gpt2_tokenizer.decode(response_tensors[0, :])\n",
    "response_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # of course no one knew what was going on when someone was up to the top of your own'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = '[positive] The movie'\n",
    "input_string += \" didn't really have a focus at all. No need to make obvious movies.\"\n",
    "input_tokens = gpt2_tokenizer.encode(input_string, return_tensors=\"pt\").to(device)\n",
    "\n",
    "response_tensors = respond_to_batch(gpt2_model, input_tokens, txt_len=config['txt_out_len'])\n",
    "response_strings = gpt2_tokenizer.decode(response_tensors[0, :])\n",
    "response_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "Finally, we save the model to disk for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2-imdb-ctrl/tokenizer_config.json',\n",
       " 'gpt2-imdb-ctrl/special_tokens_map.json',\n",
       " 'gpt2-imdb-ctrl/vocab.json',\n",
       " 'gpt2-imdb-ctrl/merges.txt',\n",
       " 'gpt2-imdb-ctrl/added_tokens.json')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.makedirs('gpt2-imdb-ctrl')\n",
    "gpt2_model.save_pretrained('gpt2-imdb-ctrl')\n",
    "gpt2_tokenizer.save_pretrained('gpt2-imdb-ctrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "10b53c383190f8e803e29126eeed82fc207de5c3c3136a21e20ff73c9f050f4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
